{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import optim\n",
    "import math\n",
    "from metric import get_mrr, get_recall\n",
    "import datetime\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "import pickle\n",
    "from entmax import  entmax_bisect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.set_device(0)\n",
    "\n",
    "test64 = pickle.load(open('data/diginetica/test.txt', 'rb'))\n",
    "train64 = pickle.load(open('data/diginetica/train.txt', 'rb'))\n",
    "\n",
    "train64_x = train64[1]\n",
    "train64_y = train64[2]\n",
    "\n",
    "test64_x = test64[1]\n",
    "test64_y = test64[2]\n",
    "train_pos = list()\n",
    "test_pos = list()\n",
    "\n",
    "item_set = set()\n",
    "item_set = set()\n",
    "\n",
    "for items in train64[1]:\n",
    "    pos = list()\n",
    "    for id_ in range(len(items)):\n",
    "        item_set.add(items[id_])\n",
    "        pos.append(id_ + 1)\n",
    "    pos.append(len(items)+1)\n",
    "    train_pos.append(pos)\n",
    "\n",
    "for item in train64[2]:\n",
    "    item_set.add(item)\n",
    "\n",
    "for items in test64[1]:\n",
    "    pos = []\n",
    "    for id_ in range(len(items)):\n",
    "        item_set.add(items[id_])\n",
    "        pos.append(id_ + 1)\n",
    "    pos.append(len(items)+1)\n",
    "    test_pos.append(pos)\n",
    "    \n",
    "for item in test64[2]:\n",
    "    item_set.add(item)\n",
    "item_list = sorted(list(item_set))\n",
    "item_dict = dict()\n",
    "for i in range(1, len(item_set)+1):\n",
    "    item = item_list[i-1]\n",
    "    item_dict[item] = i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train64_x = list()\n",
    "train64_y = list()\n",
    "\n",
    "test64_x = list()\n",
    "test64_y = list()\n",
    "    \n",
    "for items in train64[1]:\n",
    "    new_list = []\n",
    "    for item in items:\n",
    "        new_item = item_dict[item]\n",
    "        new_list.append(new_item)\n",
    "    train64_x.append(new_list)\n",
    "for item in train64[2]:\n",
    "    new_item = item_dict[item]\n",
    "    train64_y.append(new_item)\n",
    "for items in test64[1]:\n",
    "    new_list = []\n",
    "    for item in items:\n",
    "        new_item = item_dict[item]\n",
    "        new_list.append(new_item)\n",
    "    test64_x.append(new_list)\n",
    "for item in test64[2]:\n",
    "    new_item = item_dict[item]\n",
    "    test64_y.append(new_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "max_length = 0\n",
    "for sample in train64_x:\n",
    "    max_length = len(sample) if len(sample) > max_length else max_length\n",
    "for sample in test64_x:\n",
    "    max_length = len(sample) if len(sample) > max_length else max_length \n",
    "\n",
    "train_seqs = np.zeros((len(train64_x), max_length))\n",
    "train_poses = np.zeros((len(train64_x), max_length+1))\n",
    "test_seqs = np.zeros((len(test64_x), max_length))\n",
    "test_poses = np.zeros((len(test64_x), max_length+1))\n",
    "\n",
    "for i in range(len(train64_x)):\n",
    "    seq = train64_x[i]\n",
    "    pos = train_pos[i]\n",
    "    length = len(seq)\n",
    "    train_seqs[i][-length:] = seq\n",
    "    train_poses[i][-length-1:] = pos\n",
    "    \n",
    "for i in range(len(test64_x)):\n",
    "    seq = test64_x[i]\n",
    "    pos = test_pos[i]\n",
    "    length = len(seq)\n",
    "    test_seqs[i][-length:] = seq\n",
    "    test_poses[i][-length-1:] = pos\n",
    "\n",
    "target_seqs = np.array(train64_y)\n",
    "target_test_seqs = np.array(test64_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "item_set = set()\n",
    "for items in train64_x:\n",
    "    for item in items:\n",
    "        item_set.add(item)\n",
    "for item in train64_y:\n",
    "    item_set.add(item)\n",
    "for items in test64_x:\n",
    "    for item in items:\n",
    "        item_set.add(item)\n",
    "for item in test64_y:\n",
    "    item_set.add(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_x = torch.Tensor(train_seqs)\n",
    "train_pos = torch.Tensor(train_poses)\n",
    "train_y = torch.Tensor(target_seqs)\n",
    "test_x = torch.Tensor(test_seqs)\n",
    "test_pos = torch.Tensor(test_poses)\n",
    "test_y = torch.Tensor(target_test_seqs)\n",
    "train_label = torch.Tensor([40841]).repeat(len(train64_x)).unsqueeze(1)\n",
    "test_label = torch.Tensor([40841]).repeat(len(test64_x)).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_x = torch.cat((train_x, train_label), 1)\n",
    "test_x = torch.cat((test_x, test_label), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class DualAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, item_dim, pos_dim, n_items, n_pos, w, atten_way='dot', decoder_way='bilinear', dropout=0,\n",
    "                 activate='relu'):\n",
    "        super(DualAttention, self).__init__()\n",
    "        self.item_dim = item_dim\n",
    "        self.pos_dim = pos_dim\n",
    "        dim = item_dim + pos_dim\n",
    "        self.dim = dim\n",
    "        self.n_items = n_items\n",
    "        self.embedding = nn.Embedding(n_items + 1, item_dim, padding_idx=0,max_norm=1.5)\n",
    "        self.pos_embedding = nn.Embedding(n_pos, pos_dim, padding_idx=0, max_norm=1.5)\n",
    "        self.atten_way = atten_way\n",
    "        self.decoder_way = decoder_way\n",
    "        self.atten_w0 = nn.Parameter(torch.Tensor(1, dim))\n",
    "        self.atten_w1 = nn.Parameter(torch.Tensor(dim, dim))\n",
    "        self.atten_w2 = nn.Parameter(torch.Tensor(dim, dim))\n",
    "        self.atten_bias = nn.Parameter(torch.Tensor(dim))\n",
    "        self.w_f = nn.Linear(2*dim, item_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.self_atten_w1 = nn.Linear(dim, dim)\n",
    "        self.self_atten_w2 = nn.Linear(dim, dim)\n",
    "        \n",
    "        self.LN = nn.LayerNorm(dim)\n",
    "        self.LN2 = nn.LayerNorm(item_dim)\n",
    "        self.is_dropout = True\n",
    "        self.attention_mlp = nn.Linear(dim, dim)\n",
    "        self.alpha_w = nn.Linear(dim, 1)\n",
    "        self.w = w\n",
    "        \n",
    "        if activate == 'relu':\n",
    "            self.activate = F.relu\n",
    "        elif activate == 'selu':\n",
    "            self.activate = F.selu\n",
    "\n",
    "        self.initial_()\n",
    "\n",
    "    def initial_(self):\n",
    "        \n",
    "        init.normal_(self.atten_w0, 0, 0.05)\n",
    "        init.normal_(self.atten_w1, 0, 0.05)\n",
    "        init.normal_(self.atten_w2, 0, 0.05)\n",
    "        init.constant_(self.atten_bias, 0)\n",
    "        init.constant_(self.attention_mlp.bias, 0)\n",
    "        init.constant_(self.embedding.weight[0], 0)\n",
    "        init.constant_(self.pos_embedding.weight[0], 0)\n",
    "\n",
    "    def forward(self, x, pos):\n",
    "        self.is_dropout = True\n",
    "        x_embeddings = self.embedding(x)  # B,seq,dim\n",
    "        pos_embeddings = self.pos_embedding(pos)  # B, seq, dim \n",
    "        mask = (x != 0).float()  # B,seq\n",
    "        x_ = torch.cat((x_embeddings, pos_embeddings), 2)  # B seq, 2*dim\n",
    "        x_s = x_[:, :-1, :]  # B, seq-1, 2*dim\n",
    "        alpha_ent = self.get_alpha(x = x_[:, -1, :], number= 0)\n",
    "        m_s, x_n = self.self_attention(x_, x_, x_, mask, alpha_ent)\n",
    "        alpha_global = self.get_alpha(x= m_s, number=1)\n",
    "        global_c = self.global_attention(m_s, x_n, x_s, mask, alpha_global)  # B, 1, dim\n",
    "        h_t = global_c\n",
    "        result = self.decoder(h_t, m_s)\n",
    "        return result\n",
    "    \n",
    "    def get_alpha(self, x=None, number=None):\n",
    "        if number == 0:\n",
    "            alpha_ent = torch.sigmoid(self.alpha_w(x)) + 1\n",
    "            alpha_ent = self.add_value(alpha_ent).unsqueeze(1)\n",
    "            alpha_ent = alpha_ent.expand(-1, 70, -1)\n",
    "            return alpha_ent\n",
    "        if number == 1:\n",
    "            alpha_global = torch.sigmoid(self.alpha_w(x)) + 1\n",
    "            alpha_global = self.add_value(alpha_global)\n",
    "            return alpha_global\n",
    "\n",
    "    def add_value(self, value):\n",
    "\n",
    "        mask_value = (value ==1).float()\n",
    "        value = value.masked_fill(mask_value == 1, 1.00001)\n",
    "        return value\n",
    "        \n",
    "    def self_attention(self, q, k, v, mask=None, alpha_ent = 1):\n",
    "\n",
    "        if self.is_dropout:\n",
    "            q_ = self.dropout(self.activate(self.attention_mlp(q)))\n",
    "        else:\n",
    "            q_ = self.activate(self.attention_mlp(q))\n",
    "        scores = torch.matmul(q_, k.transpose(1, 2)) / math.sqrt(self.dim)\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1).expand(-1, q.size(1), -1)\n",
    "            scores = scores.masked_fill(mask == 0, -np.inf)      \n",
    "        alpha = entmax_bisect(scores, alpha_ent, dim=-1)\n",
    "\n",
    "        att_v = torch.matmul(alpha, v)  # B, seq, dim\n",
    "        if self.is_dropout:\n",
    "            att_v = self.dropout(self.self_atten_w2(self.activate(self.self_atten_w1(att_v)))) + att_v\n",
    "        else:\n",
    "            att_v = self.self_atten_w2(self.activate(self.self_atten_w1(att_v))) + att_v\n",
    "        att_v = self.LN(att_v)\n",
    "        c = att_v[:, -1, :].unsqueeze(1)\n",
    "        x_n = att_v[:, :-1, :]\n",
    "        return c, x_n\n",
    "\n",
    "    def global_attention(self,target,k, v, mask=None, alpha_ent=1):\n",
    "        alpha = torch.matmul(\n",
    "            torch.relu(k.matmul(self.atten_w1) + target.matmul(self.atten_w2) + self.atten_bias),\n",
    "            self.atten_w0.t())  # (B,seq,1)\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(-1)\n",
    "            mask = mask[:, :-1, :]\n",
    "            alpha = alpha.masked_fill(mask == 0, -np.inf)\n",
    "        alpha = entmax_bisect(alpha, alpha_ent, dim=1)\n",
    "        c = torch.matmul(alpha.transpose(1, 2), v)  # (B, 1, dim)\n",
    "        return c\n",
    "\n",
    "    def decoder(self, global_c, self_c):\n",
    "        if self.is_dropout:\n",
    "            c = self.dropout(torch.selu(self.w_f(torch.cat((global_c, self_c), 2))))\n",
    "        else:\n",
    "            c = torch.selu(self.w_f(torch.cat((global_c, self_c), 2)))\n",
    "        c = c.squeeze()\n",
    "        l_c = (c/torch.norm(c, dim=-1).unsqueeze(1))\n",
    "        l_emb = self.embedding.weight[1:-1]/torch.norm(self.embedding.weight[1:-1], dim=-1).unsqueeze(1)\n",
    "        z = self.w * torch.matmul(l_c, l_emb.t())\n",
    "\n",
    "        return z\n",
    "\n",
    "\n",
    "    def predict(self, x, pos, k=20):\n",
    "        self.is_dropout = False\n",
    "        x_embeddings = self.embedding(x)  # B,seq,dim\n",
    "        pos_embeddings = self.pos_embedding(pos)  # B, seq, dim\n",
    "        mask = (x != 0).float()  # B,seq\n",
    "        x_ = torch.cat((x_embeddings, pos_embeddings), 2)  # B seq, 2*dim\n",
    "        x_s = x_[:, :-1, :]  # B, seq-1, 2*dim\n",
    "        alpha_ent = self.get_alpha(x = x_[:, -1, :], number= 0)\n",
    "        m_s, x_n = self.self_attention(x_, x_, x_, mask, alpha_ent)\n",
    "        alpha_global = self.get_alpha(x= m_s, number=1)\n",
    "        global_c = self.global_attention(m_s, x_n, x_s, mask, alpha_global)  # B, 1, dim\n",
    "        h_t = global_c\n",
    "        result = self.decoder(h_t, m_s)\n",
    "        rank = torch.argsort(result, dim=1, descending=True)\n",
    "        return rank[:, 0:k]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "w_list = [20]\n",
    "record = list()\n",
    "for w in w_list:\n",
    "    np.random.seed(1)\n",
    "    torch.manual_seed(1)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "    train_sets = TensorDataset(train_x.long(), train_pos.long(), train_y.long())\n",
    "    train_dataload = DataLoader(train_sets, batch_size=512, shuffle=True)\n",
    "    criterion = nn.CrossEntropyLoss().cuda()\n",
    "    test_x, test_pos, test_y = test_x.long(), test_pos.long(), test_y.long()\n",
    "    all_test_sets = TensorDataset(test_x, test_pos, test_y)\n",
    "    test_dataload = DataLoader(all_test_sets, batch_size=512,shuffle=False)\n",
    "    model = DualAttention(100, 100, 40842, 71, w, dropout=0.5, activate='relu').cuda()\n",
    "    opti = optim.Adam(model.parameters(), lr=0.001, weight_decay=0, amsgrad=True)\n",
    "    best_result = 0\n",
    "    total_time = 0\n",
    "    best_result_5 = 0\n",
    "    best_result_ = []\n",
    "#     for epoch in range(50):\n",
    "#         start_time = datetime.datetime.now()\n",
    "#         print(datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "#         losses = 0\n",
    "#         for step, (x_train, pos_train, y_train) in enumerate(train_dataload):\n",
    "#             opti.zero_grad()\n",
    "#             q = model(x_train.cuda(), pos_train.cuda())\n",
    "#             loss = criterion(q, y_train.cuda()-1)\n",
    "#             loss.backward()\n",
    "#             opti.step()\n",
    "#             losses += loss.item()\n",
    "#             if (step + 1) % 100 == 0:\n",
    "#                 print(\"[%02d/%d] [%03d/%d] mean_loss : %0.2f\" % (epoch, 50, step, len(train_sets) / 512, losses / step + 1))\n",
    "#         end_time = datetime.datetime.now()\n",
    "#         with torch.no_grad():\n",
    "#             y_pre_all = torch.LongTensor().cuda()\n",
    "#             y_pre_all_10 = torch.LongTensor()\n",
    "#             y_pre_all_5 = torch.LongTensor()\n",
    "#             for x_test, pos_test, y_test in test_dataload:\n",
    "#                 with torch.no_grad():\n",
    "#                     y_pre = model.predict(x_test.cuda(), pos_test.cuda(), 20)\n",
    "#                     y_pre_all = torch.cat((y_pre_all, y_pre), 0)\n",
    "#                     y_pre_all_10 = torch.cat((y_pre_all_10, y_pre.cpu()[:, :10]), 0)\n",
    "#                     y_pre_all_5 = torch.cat((y_pre_all_5, y_pre.cpu()[:, :5]), 0)\n",
    "#             recall = get_recall(y_pre_all, test_y.cuda().unsqueeze(1)-1)\n",
    "#             recall_10 = get_recall(y_pre_all_10, test_y.unsqueeze(1)-1)\n",
    "#             recall_5 = get_recall(y_pre_all_5, test_y.unsqueeze(1)-1)\n",
    "#             mrr = get_mrr(y_pre_all, test_y.cuda().unsqueeze(1)-1)\n",
    "#             mrr_10 = get_mrr(y_pre_all_10, test_y.unsqueeze(1)-1)\n",
    "#             mrr_5 = get_mrr(y_pre_all_5, test_y.unsqueeze(1)-1)\n",
    "#     \n",
    "#             print(\"Recall@20: \" + \"%.4f\" %recall + \" Recall@10: \" + \"%.4f\" %recall_10 +\"  Recall@5:\" + \"%.4f\" %recall_5)\n",
    "#             print(\"MRR@20:\" + \"%.4f\" % mrr.tolist() + \" MRR@10:\" + \"%.4f\" % mrr_10.tolist() + \" MRR@5:\" + \"%.4f\" % mrr_5.tolist())\n",
    "#             if best_result < recall:\n",
    "#                 best_result = recall\n",
    "#                 best_result_ = [recall_5, recall_10, recall, mrr_5, mrr_10, mrr]\n",
    "#                 torch.save(model.state_dict(), 'BestModel/best_dn_w_%s.pth' % str(w))\n",
    "#             print(\"best result: \" + str(best_result))\n",
    "#             print(\"==================================\")\n",
    "#     record.append(best_result_)\n",
    "# print(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 14
    }
   ],
   "source": [
    "model = DualAttention(100, 100, 40842, 71, 20, atten_way='MLP', decoder_way='trilinear2', dropout=0.5, activate='relu').cuda()\n",
    "model.load_state_dict(torch.load('BestModel/best_dn_w_20.pth'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Recall@20: 0.5376 Recall@10: 0.4029  Recall@5:0.2869\n",
      "MRR@20:0.1899 MRR@10:0.1805 MRR@5:0.1651\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": [
      "/home/cbd109-3/Users/YJH/DSANForAAAI2021/metric.py:16: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero()\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(*, bool as_tuple) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:882.)\n",
      "  hits = (pre == truths).nonzero()\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    y_pre_all = torch.LongTensor().cuda()\n",
    "    y_pre_all_10 = torch.LongTensor()\n",
    "    y_pre_all_5 = torch.LongTensor()\n",
    "    for x_test, pos_test, y_test in test_dataload:\n",
    "        with torch.no_grad():\n",
    "            y_pre = model.predict(x_test.cuda(), pos_test.cuda(), 20)\n",
    "            y_pre_all = torch.cat((y_pre_all, y_pre), 0)\n",
    "            y_pre_all_10 = torch.cat((y_pre_all_10, y_pre.cpu()[:, :10]), 0)\n",
    "            y_pre_all_5 = torch.cat((y_pre_all_5, y_pre.cpu()[:, :5]), 0)\n",
    "    recall = get_recall(y_pre_all, test_y.cuda().unsqueeze(1)-1)\n",
    "    recall_10 = get_recall(y_pre_all_10, test_y.unsqueeze(1)-1)\n",
    "    recall_5 = get_recall(y_pre_all_5, test_y.unsqueeze(1)-1)\n",
    "    mrr = get_mrr(y_pre_all, test_y.cuda().unsqueeze(1)-1)\n",
    "    mrr_10 = get_mrr(y_pre_all_10, test_y.unsqueeze(1)-1)\n",
    "    mrr_5 = get_mrr(y_pre_all_5, test_y.unsqueeze(1)-1)\n",
    "\n",
    "    print(\"Recall@20: \" + \"%.4f\" %recall + \" Recall@10: \" + \"%.4f\" %recall_10 +\"  Recall@5:\" + \"%.4f\" %recall_5)\n",
    "    print(\"MRR@20:\" + \"%.4f\" % mrr.tolist() + \" MRR@10:\" + \"%.4f\" % mrr_10.tolist() + \" MRR@5:\" + \"%.4f\" % mrr_5.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "yjh",
   "language": "python",
   "display_name": "yjh"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}